{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/yjkim/Developer/WhatsUpDoc/rag-notebooks/venv/lib/python3.11/site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqr ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase.client import Client, create_client\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.supabase import SupabaseVectorStore\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "supabase_client = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    embedding=embeddings,\n",
    "    table_name=f\"langchain_documents\",\n",
    "    query_name=f\"match_langchain_documents\",\n",
    ")\n",
    "\n",
    "retriever = supabase_client.as_retriever(search_type='mmr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 14:26:27,026:INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-03-01 14:26:28,627:INFO - HTTP Request: POST https://qublpyarwoevdeqqicbz.supabase.co/rest/v1/rpc/match_langchain_documents?limit=6 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"'s support for streaming, parallel operations, and async, and LCEL's nice integration with LangChain's focus on chaining components together.Some people are less fond of LCEL. These people typically point to LCEL being yet another abstraction on top of an already very abstract library, that the syntax is confusing, against the Zen of Python, and requires too much effort to learn new (or uncommon) syntax.Both viewpoints are entirely valid, LCEL is a very different approach ‚Äî there are major pros and major cons. In either case, if you're willing to spend some time learning the syntax, it allows us to develop very quickly, and with that in mind, it's well worth learning.Share via:  PreviousAgents With Long-Term MemoryLangChain AI HandbookChaptersAn Introduction to LangChainPrompt Templates and the Art of PromptsBuilding Composable Pipelines with Chains Conversational MemoryRetrieval AugmentationAI AgentsCustom ToolsAgents With Long-Term Memory Streaming in LangChain RAG Multi-Query LangCh\", metadata={'title': 'LangChain Expression Language Explained | Pinecone', 'source': 'https://www.pinecone.io/learn/series/langchain/langchain-expression-language/', 'language': 'en'}),\n",
       " Document(page_content='either method, we get the same response, and at its core, this is the pipe logic that LCEL uses when chaining together components. However, this is not all there is to LCEL, there is more.LCEL Deep DiveNow that we understand what the LCEL syntax is doing under the hood, let\\'s explore it within the context of LangChain and see a few of the additional methods that are provided to maximize flexibility when working with LCEL.RunnablesWhen working with LCEL we may find that we need to modify the flow of values, or the values themselves as they are passed between components ‚Äî for this, we can use runnables. Let\\'s begin by initializing a couple of simple vector store components.from langchain.embeddings import CohereEmbeddings\\nfrom langchain.vectorstores import DocArrayInMemorySearch\\n\\nCOHERE_API_KEY = \"<<COHERE_API_KEY>>\"\\n\\nembedding = CohereEmbeddings(\\n    model=\"embed-english-light-v3.0\",\\n    cohere_api_key=COHERE_API_KEY\\n)\\n\\nvecstore_a = DocArrayInMemorySearch.from_texts(\\n    [\"half the info', metadata={'title': 'LangChain Expression Language Explained | Pinecone', 'source': 'https://www.pinecone.io/learn/series/langchain/langchain-expression-language/', 'language': 'en'}),\n",
       " Document(page_content='LangChain Expression Language Explained | PineconeLangChain Expression Language ExplainedJump to section LCEL SyntaxHow the Pipe Operator WorksLCEL Deep DiveRunnable LambdasThe LangChain Expression Language (LCEL) is an abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.LCEL comes with strong support for:Superfast development of chains.Advanced features such as streaming, async, parallel execution, and more.Easy integration with LangSmith and LangServe.In this article, we\\'ll learn what LCEL is, how it works, and the essentials of LCEL chains, pipes, and Runnables.LCEL SyntaxWe\\'ll begin by installing all of the prerequisite libraries that we\\'ll need for this walkthrough. Note, you can follow along via our Jupyter notebook here.!pip install -qU \\\\\\n    langchain==0.0.345 \\\\\\n    anthropic==0.7.7 \\\\\\n    cohere==4.37 \\\\\\n    docarray==0.39.1To understand LCEL syntax let\\'s first build a simple chain using', metadata={'title': 'LangChain Expression Language Explained | Pinecone', 'source': 'https://www.pinecone.io/learn/series/langchain/langchain-expression-language/', 'language': 'en'}),\n",
       " Document(page_content='3\\n \\n\\n \\n\\n1\\n You must be logged in to vote\\n\\nüôè\\n\\n              How to stream to langserve with a HuggingFacePipeline and LCEL\\n              \\n\\nmojoee\\n            \\n            asked\\n            Feb 29, 2024\\n in \\nQ&A\\n\\n                  ¬∑ Unanswered\\n                \\n\\n \\n\\n              2\\n \\n\\n \\n\\n1\\n You must be logged in to vote\\n\\nüôè\\n\\n              payload error on custom agent\\n              \\n\\nadream307\\n            \\n            asked\\n            Feb 29, 2024\\n in \\nQ&A\\n\\n                  ¬∑ Unanswered\\n                \\n\\n \\n\\n              3\\n \\n\\n \\n\\n1\\n You must be logged in to vote\\n\\nüôè\\n\\n              How to pass arguments to a function in LCEL\\n              \\n\\nmingovvv\\n            \\n            asked\\n            Feb 29, 2024\\n in \\nQ&A\\n\\n                  ¬∑ Unanswered\\n                \\n\\n \\n\\n              1\\n \\n\\n \\n\\n1\\n You must be logged in to vote\\n\\nüôè\\n\\n              Issue with GraphQAChain: Inability to Recognize Context Beyond 2-3 Graph Triplets\\n              \\n\\nMNLubov\\n            \\n            asked', metadata={'title': 'langchain-ai/langchain Q A ¬∑ Discussions ¬∑ GitHub', 'source': 'https://github.com/langchain-ai/langchain/discussions/categories/q-a', 'language': 'en', 'description': 'Explore the GitHub Discussions forum for langchain-ai langchain in the Q A category.'}),\n",
       " Document(page_content='via our Jupyter notebook here.!pip install -qU \\\\\\n    langchain==0.0.345 \\\\\\n    anthropic==0.7.7 \\\\\\n    cohere==4.37 \\\\\\n    docarray==0.39.1To understand LCEL syntax let\\'s first build a simple chain using the traditional LangChain syntax. We will initialize a simple LLMChain using Claude 2.1.from langchain.chat_models import ChatAnthropic\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema.output_parser import StrOutputParser\\n\\nANTHROPIC_API_KEY = \"<<YOUR_ANTHROPIC_API_KEY>>\"\\n\\nprompt = ChatPromptTemplate.from_template(\\n    \"Give me small report about {topic}\"\\n)\\nmodel = ChatAnthropic(\\n    model=\"claude-2.1\",\\n    max_tokens_to_sample=512,\\n    anthropic_api_key=ANTHROPIC_API_KEY\\n)  # swap Anthropic for OpenAI with `ChatOpenAI` and `openai_api_key`\\noutput_parser = StrOutputParser()Using this chain we can generate a small report about a particular topic, such as \"Artificial Intelligence\" by calling the chain.run method on an LLMChain:With LCEL we create our chain differently u', metadata={'title': 'LangChain Expression Language Explained | Pinecone', 'source': 'https://www.pinecone.io/learn/series/langchain/langchain-expression-language/', 'language': 'en'}),\n",
       " Document(page_content='How to pass arguments to a function in LCEL ¬∑ langchain-ai/langchain ¬∑ Discussion #18311 ¬∑ GitHub\\n\\n        langchain-ai\\n \\n/\\n\\nlangchain\\n\\nPublic\\n\\n \\n\\nNotifications\\n\\n \\n\\nFork\\n    11.7k\\n\\n \\n\\n          Star\\n 77.9k\\n  \\n\\nCode\\n\\nIssues\\n1.3k\\n\\nPull requests\\n489\\n\\nDiscussions\\n\\nActions\\n\\nProjects\\n1\\n\\nSecurity\\n\\nInsights\\n\\n \\n\\n \\n\\nAdditional navigation options\\n\\n \\n\\n          Code\\n\\n          Issues\\n\\n          Pull requests\\n\\n          Discussions\\n\\n          Actions\\n\\n          Projects\\n\\n          Security\\n\\n          Insights\\n\\n \\n\\n              How to pass arguments to a function in LCEL\\n            \\n#18311\\n\\nUnanswered\\n\\nmingovvv\\n\\n              \\n                asked this question in\\n              Q&A\\n\\n                How to pass arguments to a function in LCEL\\n              \\n#18311\\n\\nmingovvv\\n\\nFeb 29, 2024\\n¬∑\\n    1 comment\\n  \\n\\nReturn to top\\n\\n \\n\\nDiscussion options\\n\\n \\nQuote reply\\n\\nmingovvv\\n\\nFeb 29, 2024\\n\\n \\n\\n -\\n           \\n \\n\\nChecked other resources\\n\\n I added a very descriptive title to this question.\\n I searched the Lan', metadata={'title': 'How to pass arguments to a function in LCEL ¬∑ langchain-ai/langchain ¬∑ Discussion #18311 ¬∑ GitHub', 'source': 'https://github.com/langchain-ai/langchain/discussions/18311', 'language': 'en', 'description': 'How to pass arguments to a function in LCEL'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is LCEL?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
